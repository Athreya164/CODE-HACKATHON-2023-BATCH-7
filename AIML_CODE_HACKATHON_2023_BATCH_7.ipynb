{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2QuUuDNQ0sf",
        "outputId": "4cf417fc-5d5f-4155-833d-6b73e4e6e09a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "RnWPkajTe5bb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JljNbHvAvna",
        "outputId": "63ed0178-25e7-449d-ebb1-76c67c72ca81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              "  , Dave, watched, as, the, forest, burned, up, on, the, hill, ,, \n",
              "  , only, a, few, miles, from, his, house, ., The, car, had, \n",
              "  , been, hastily, packed, and, Marta, was, inside, trying, to, round, \n",
              "  , up, the, last, of, the, pets, ., \", Where, could, she, be, ?, \", he, wondered, \n",
              "  , as, he, continued, to, wait, for, Marta, to, appear, with, the, pets, ., \n",
              "  ]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "#TOKENIZING\n",
        "import spacy\n",
        "text = \"\"\"\n",
        " Dave watched as the forest burned up on the hill,\n",
        " only a few miles from his house. The car had\n",
        " been hastily packed and Marta was inside trying to round\n",
        " up the last of the pets. \"Where could she be?\" he wondered\n",
        " as he continued to wait for Marta to appear with the pets.\n",
        " \"\"\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "token_list = [token for token in doc]\n",
        "token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvV48BoLA5_A",
        "outputId": "008960e9-ca08-490b-b673-ac01d194abb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              "  , Dave, watched, forest, burned, hill, ,, \n",
              "  , miles, house, ., car, \n",
              "  , hastily, packed, Marta, inside, trying, round, \n",
              "  , pets, ., \", ?, \", wondered, \n",
              "  , continued, wait, Marta, appear, pets, ., \n",
              "  ]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "#REMOVING STOP WORDS\n",
        "filtered_tokens = [token for token in doc if not token.is_stop]\n",
        "filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ24nyy2BH1p",
        "outputId": "5d35fa1a-bb79-489f-9e0a-4a4ff34e7080"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Token: \\n , lemma: \\n ',\n",
              " 'Token: Dave, lemma: Dave',\n",
              " 'Token: watched, lemma: watch',\n",
              " 'Token: forest, lemma: forest',\n",
              " 'Token: burned, lemma: burn',\n",
              " 'Token: hill, lemma: hill',\n",
              " 'Token: ,, lemma: ,',\n",
              " 'Token: \\n , lemma: \\n ',\n",
              " 'Token: miles, lemma: mile',\n",
              " 'Token: house, lemma: house',\n",
              " 'Token: ., lemma: .',\n",
              " 'Token: car, lemma: car',\n",
              " 'Token: \\n , lemma: \\n ',\n",
              " 'Token: hastily, lemma: hastily',\n",
              " 'Token: packed, lemma: pack',\n",
              " 'Token: Marta, lemma: Marta',\n",
              " 'Token: inside, lemma: inside',\n",
              " 'Token: trying, lemma: try',\n",
              " 'Token: round, lemma: round',\n",
              " 'Token: \\n , lemma: \\n ',\n",
              " 'Token: pets, lemma: pet',\n",
              " 'Token: ., lemma: .',\n",
              " 'Token: \", lemma: \"',\n",
              " 'Token: ?, lemma: ?',\n",
              " 'Token: \", lemma: \"',\n",
              " 'Token: wondered, lemma: wonder',\n",
              " 'Token: \\n , lemma: \\n ',\n",
              " 'Token: continued, lemma: continue',\n",
              " 'Token: wait, lemma: wait',\n",
              " 'Token: Marta, lemma: Marta',\n",
              " 'Token: appear, lemma: appear',\n",
              " 'Token: pets, lemma: pet',\n",
              " 'Token: ., lemma: .',\n",
              " 'Token: \\n , lemma: \\n ']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "#NORMALIZING WORDS\n",
        "lemmas = [\n",
        "     f\"Token: {token}, lemma: {token.lemma_}\"\n",
        "     for token in filtered_tokens\n",
        "]\n",
        "lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csMD4nGGBH8R",
        "outputId": "0e6a2b54-69b1-4d4a-9aaa-c0d49f25d836"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.22178409,  2.0363135 ,  1.1723998 ,  0.18578127,  0.04702622,\n",
              "        0.58668554, -1.2009034 ,  0.3269077 ,  1.4433299 , -1.4248612 ,\n",
              "       -0.21548331,  0.42028397, -1.3350626 , -0.73203874, -0.31279454,\n",
              "       -0.13874963,  0.3950568 ,  0.12919296, -0.43140125, -0.34982377,\n",
              "       -0.05765158, -1.1015071 ,  0.27901345,  0.84183687,  1.3245786 ,\n",
              "       -0.8529462 , -0.8744372 , -1.0205163 , -0.9564352 ,  1.0853741 ,\n",
              "       -0.07206851,  0.40644735, -0.498676  , -0.92382216,  0.3699388 ,\n",
              "        0.29741818,  0.48972166, -0.12200204, -0.08074802,  0.76989526,\n",
              "       -0.9041327 , -0.75449955, -0.04722948, -2.0991108 , -0.77466094,\n",
              "        0.02502659,  0.7373609 ,  0.62764734,  0.81472605, -0.5014193 ,\n",
              "        0.79458493, -0.60294414, -0.3764605 , -1.7542655 ,  0.04777254,\n",
              "        1.7780645 , -0.24561684, -0.7471707 , -0.8752783 , -0.9624736 ,\n",
              "       -1.2766806 , -0.65242827,  0.26527596, -0.22189498,  0.69737744,\n",
              "        0.17414027, -0.11381237,  0.1534035 ,  0.773933  , -0.83953214,\n",
              "        0.01538646,  0.18135247, -1.4819458 ,  0.1674507 , -0.7085006 ,\n",
              "       -0.67284477, -0.5845107 , -0.33679256, -0.7437197 ,  1.0022056 ,\n",
              "       -0.09139998, -1.0485872 ,  0.26224685,  2.7282083 ,  1.4432502 ,\n",
              "        0.7343503 , -1.0600086 ,  2.6718006 ,  0.09900571,  0.15239403,\n",
              "       -0.7016692 , -1.3322375 ,  1.3609853 ,  1.2155815 ,  0.12229937,\n",
              "        2.356394  ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "#VECTORIZING THE TEXT\n",
        "filtered_tokens[1].vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "k6t2X35rCfPB"
      },
      "outputs": [],
      "source": [
        "#LOADING DATA AND PREPROCESSING\n",
        "import os\n",
        "\n",
        "def load_training_data(\n",
        "    data_directory: str = \"aclImdb/train\",\n",
        "    split: float = 0.8,\n",
        "    limit: int = 0\n",
        ") -> tuple:\n",
        "    # Load from files\n",
        "    reviews = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        labeled_directory = f\"{data_directory}/{label}\"\n",
        "        for review in os.listdir(labeled_directory):\n",
        "            if review.endswith(\".txt\"):\n",
        "                with open(f\"{labeled_directory}/{review}\") as f:\n",
        "                    text = f.read()\n",
        "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
        "                    if text.strip():\n",
        "                        spacy_label = {\n",
        "                            \"cats\": {\n",
        "                                \"pos\": \"pos\" == label,\n",
        "                                \"neg\": \"neg\" == label\n",
        "                            }\n",
        "                        }\n",
        "                        reviews.append((text, spacy_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Sb7GijamBID2"
      },
      "outputs": [],
      "source": [
        "#SHUFFLE YOUR DATA\n",
        "import os\n",
        "import random\n",
        "\n",
        "def load_training_data(\n",
        "    data_directory: str = \"aclImdb/train\",\n",
        "    split: float = 0.8,\n",
        "    limit: int = 0\n",
        ") -> tuple:\n",
        "    # Load from files\n",
        "    reviews = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        labeled_directory = f\"{data_directory}/{label}\"\n",
        "        for review in os.listdir(labeled_directory):\n",
        "            if review.endswith(\".txt\"):\n",
        "                with open(f\"{labeled_directory}/{review}\") as f:\n",
        "                    text = f.read()\n",
        "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
        "                    if text.strip():\n",
        "                        spacy_label = {\n",
        "                            \"cats\": {\n",
        "                                \"pos\": \"pos\" == label,\n",
        "                                \"neg\": \"neg\" == label}\n",
        "                        }\n",
        "                        reviews.append((text, spacy_label))\n",
        "    random.shuffle(reviews)\n",
        "\n",
        "    if limit:\n",
        "        reviews = reviews[:limit]\n",
        "    split = int(len(reviews) * split)\n",
        "    return reviews[:split], reviews[split:]\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUENo7P1DM5O"
      },
      "source": [
        "TRAIN YOUR CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "SKmvDrbhBIGR"
      },
      "outputs": [],
      "source": [
        "#ADD TEXTCAT\n",
        "import os\n",
        "import random\n",
        "import spacy\n",
        "\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 20\n",
        ") -> None:\n",
        "    # Build pipeline\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "QSxqWstTDL_-"
      },
      "outputs": [],
      "source": [
        "#ADD TEXTCAT AND ADD LABELS\n",
        "import os\n",
        "import random\n",
        "import spacy\n",
        "\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 20\n",
        ") -> None:\n",
        "    # Build pipeline\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    textcat.add_label(\"pos\")\n",
        "    textcat.add_label(\"neg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "e3HCq78BDMCu"
      },
      "outputs": [],
      "source": [
        "#IMPLEMENT TRAINING LOOP\n",
        "import os\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 20\n",
        ") -> None:\n",
        "    # Build pipeline\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    textcat.add_label(\"pos\")\n",
        "    textcat.add_label(\"neg\")\n",
        "\n",
        "    # Train only textcat\n",
        "    training_excluded_pipes = [\n",
        "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCM91jpEDdp4"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "cc_jbQLyDMFu"
      },
      "outputs": [],
      "source": [
        "#ADD CODE TO BEGIN TRAINING\n",
        "import os\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 20\n",
        ") -> None:\n",
        "    # Build pipeline\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    textcat.add_label(\"pos\")\n",
        "    textcat.add_label(\"neg\")\n",
        "\n",
        "    # Train only textcat\n",
        "    training_excluded_pipes = [\n",
        "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
        "    ]\n",
        "    with nlp.disable_pipes(training_excluded_pipes):\n",
        "        optimizer = nlp.begin_training()\n",
        "        # Training loop\n",
        "        print(\"Beginning training\")\n",
        "        batch_sizes = compounding(\n",
        "            4.0, 32.0, 1.001\n",
        "        )  # A generator that yields infinite series of input numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "KWsmAeaCDMIO"
      },
      "outputs": [],
      "source": [
        "#TRAIN BATCHES OF DATA\n",
        "import os\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 20\n",
        ") -> None:\n",
        "    # Build pipeline\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    textcat.add_label(\"pos\")\n",
        "    textcat.add_label(\"neg\")\n",
        "\n",
        "    # Train only textcat\n",
        "    training_excluded_pipes = [\n",
        "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
        "    ]\n",
        "    with nlp.disable_pipes(training_excluded_pipes):\n",
        "        optimizer = nlp.begin_training()\n",
        "        # Training loop\n",
        "        print(\"Beginning training\")\n",
        "        batch_sizes = compounding(\n",
        "            4.0, 32.0, 1.001\n",
        "        )  # A generator that yields infinite series of input numbers\n",
        "        for i in range(iterations):\n",
        "            loss = {}\n",
        "            random.shuffle(training_data)\n",
        "            batches = minibatch(training_data, size=batch_sizes)\n",
        "            for batch in batches:\n",
        "                text, labels = zip(*batch)\n",
        "                nlp.update(\n",
        "                    text,\n",
        "                    labels,\n",
        "                    drop=0.2,\n",
        "                    sgd=optimizer,\n",
        "                    losses=loss\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZYPxpRD0-o"
      },
      "source": [
        "EVALUATING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "2B8LKcmYDMK1"
      },
      "outputs": [],
      "source": [
        "#PASS THE REQUIRED COMPONENTS\n",
        "def evaluate_model(\n",
        "    tokenizer, textcat, test_data: list\n",
        ") -> dict:\n",
        "    reviews, labels = zip(*test_data)\n",
        "    reviews = (tokenizer(review) for review in reviews)\n",
        "    true_positives = 0\n",
        "    false_positives = 1e-8  # Can't be 0 because of presence in denominator\n",
        "    true_negatives = 0\n",
        "    false_negatives = 1e-8\n",
        "    for i, review in enumerate(textcat.pipe(reviews)):\n",
        "        true_label = labels[i]\n",
        "        for predicted_label, score in review.cats.items():\n",
        "            # Every cats dictionary includes both labels. You can get all\n",
        "            # the info you need with just the pos label.\n",
        "            if (\n",
        "                predicted_label == \"neg\"\n",
        "            ):\n",
        "                continue\n",
        "            if score >= 0.5 and true_label[\"pos\"]:\n",
        "                true_positives += 1\n",
        "            elif score >= 0.5 and true_label[\"neg\"]:\n",
        "                false_positives += 1\n",
        "            elif score < 0.5 and true_label[\"neg\"]:\n",
        "                true_negatives += 1\n",
        "            elif score < 0.5 and true_label[\"pos\"]:\n",
        "                false_negatives += 1\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        f_score = 0\n",
        "    else:\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "a5mZaaG8DMNN"
      },
      "outputs": [],
      "source": [
        "#DETERMINE PERFORMANCE METRICS LIKE PRECISION RECALL AND F SCORE\n",
        "def train_model(training_data: list, test_data: list, iterations: int = 20):\n",
        "    # Previously seen code omitted for brevity.\n",
        "        # Training loop\n",
        "        print(\"Beginning training\")\n",
        "        print(\"Loss\\tPrecision\\tRecall\\tF-score\")\n",
        "        batch_sizes = compounding(\n",
        "            4.0, 32.0, 1.001\n",
        "        )  # A generator that yields infinite series of input numbers\n",
        "        for i in range(iterations):\n",
        "            loss = {}\n",
        "            random.shuffle(training_data)\n",
        "            batches = minibatch(training_data, size=batch_sizes)\n",
        "            for batch in batches:\n",
        "                text, labels = zip(*batch)\n",
        "                nlp.update(\n",
        "                    text,\n",
        "                    labels,\n",
        "                    drop=0.2,\n",
        "                    sgd=optimizer,\n",
        "                    losses=loss\n",
        "                )\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                evaluation_results = evaluate_model(\n",
        "                    tokenizer=nlp.tokenizer,\n",
        "                    textcat=textcat,\n",
        "                    test_data=test_data\n",
        "                )\n",
        "                print(\n",
        "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
        "                    f\"\\t{evaluation_results['recall']}\"\n",
        "                    f\"\\t{evaluation_results['f-score']}\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "kZ4roIB6Ee2Z"
      },
      "outputs": [],
      "source": [
        "#COMPLETE TRAINING AND SAVE MODEL\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 20\n",
        ") -> None:\n",
        "    # Build pipeline\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    textcat.add_label(\"pos\")\n",
        "    textcat.add_label(\"neg\")\n",
        "\n",
        "    # Train only textcat\n",
        "    training_excluded_pipes = [\n",
        "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
        "    ]\n",
        "    with nlp.disable_pipes(training_excluded_pipes):\n",
        "        optimizer = nlp.begin_training()\n",
        "        # Training loop\n",
        "        print(\"Beginning training\")\n",
        "        print(\"Loss\\tPrecision\\tRecall\\tF-score\")\n",
        "        batch_sizes = compounding(\n",
        "            4.0, 32.0, 1.001\n",
        "        )  # A generator that yields infinite series of input numbers\n",
        "        for i in range(iterations):\n",
        "            print(f\"Training iteration {i}\")\n",
        "            loss = {}\n",
        "            random.shuffle(training_data)\n",
        "            batches = minibatch(training_data, size=batch_sizes)\n",
        "            for batch in batches:\n",
        "                text, labels = zip(*batch)\n",
        "                nlp.update(text, labels, drop=0.2, sgd=optimizer, losses=loss)\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                evaluation_results = evaluate_model(\n",
        "                    tokenizer=nlp.tokenizer,\n",
        "                    textcat=textcat,\n",
        "                    test_data=test_data\n",
        "                )\n",
        "                print(\n",
        "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
        "                    f\"\\t{evaluation_results['recall']}\"\n",
        "                    f\"\\t{evaluation_results['f-score']}\"\n",
        "                )\n",
        "\n",
        "    # Save model\n",
        "    with nlp.use_params(optimizer.averages):\n",
        "        nlp.to_disk(\"SENTIMENT_ANALYSIS_MODEL_BATCH 7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjKdAlxtEsm3"
      },
      "source": [
        "CLASSIFYING REVIEWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "a4eCm0ySEpv3"
      },
      "outputs": [],
      "source": [
        "#LOAD THE SAVED MODEL\n",
        "def test_model(input_data: str=\"TEST_REVIEW\"):\n",
        "    #  Load saved trained model\n",
        "    loaded_model = spacy.load(\"SENTIMENT_ANALYSIS_MODEL_BATCH 7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "QsaV9zWfEp2n"
      },
      "outputs": [],
      "source": [
        "#INSERT EXAMPLE INPUT\n",
        "import os\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "TEST_REVIEW = \"\"\"\n",
        "Transcendently beautiful in moments outside the office, it seems almost\n",
        "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
        "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
        "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
        "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "QpPexl6KEp5H"
      },
      "outputs": [],
      "source": [
        "#PASS REVIEW INTO THE MODEL\n",
        "def test_model(input_data: str = TEST_REVIEW):\n",
        "    #  Load saved trained model\n",
        "    loaded_model = spacy.load(\"SENTIMENT_ANALYSIS_MODEL_BATCH 7\")\n",
        "    # Generate prediction\n",
        "    parsed_text = loaded_model(input_data)\n",
        "    # Determine prediction to return\n",
        "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
        "        prediction = \"Positive\"\n",
        "        score = parsed_text.cats[\"pos\"]\n",
        "    else:\n",
        "        prediction = \"Negative\"\n",
        "        score = parsed_text.cats[\"neg\"]\n",
        "    print(\n",
        "        f\"Review text: {input_data}\\nPredicted sentiment: {prediction}\"\n",
        "        f\"\\tScore: {score}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB8G7AmXPxAs",
        "outputId": "c2579fd3-0f53-4a1d-dc79-0bd8a5ff864d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning training\n",
            "Loss\tPrecision\tRecall\tF-score\n",
            "11.293997120810673\t0.7816593886121546\t0.7584745762390477\t0.7698924730851658\n",
            "1.979159922178951\t0.8083333332996527\t0.8220338982702527\t0.8151260503859189\n",
            "[...]\n",
            "0.000415042785704145\t0.7926829267970453\t0.8262711864056664\t0.8091286306718204\n",
            "\n",
            "Testing model\n",
            "Review text:\n",
            "Transcendently beautiful in moments outside the office, it seems almost\n",
            "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
            "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
            "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
            "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
            "\n",
            "Predicted sentiment: Positive   Score: 0.8773064017295837\n"
          ]
        }
      ],
      "source": [
        "#CALL THE MAIN FUNCTION\n",
        "if __name__ == \"__main__\":\n",
        "    train, test = load_training_data(limit=2500)\n",
        "    train_model(train, test)\n",
        "    print(\"Testing model\")\n",
        "    test_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}